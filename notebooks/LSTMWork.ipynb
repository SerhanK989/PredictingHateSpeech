{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn import feature_extraction, linear_model, model_selection, metrics\n",
    "from sklearn import ensemble\n",
    "from scipy import sparse\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.make_model_lstm import hate_speech_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#watch overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweet'].apply(lambda x: x.lstrip('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(df[['hate_speech', 'offensive_language', 'neither']].values, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.tokenize.casual.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "wordDict = {}\n",
    "i = 1\n",
    "for tweet in tweets:\n",
    "    tokenized = token.tokenize(tweet)\n",
    "    newSent = []\n",
    "    for word in tokenized:\n",
    "        if word not in stop_words:\n",
    "            newWord = lemmatizer.lemmatize(word)\n",
    "            if newWord not in wordDict:\n",
    "                wordDict[newWord] = i\n",
    "                i += 1\n",
    "            newSent.append(newWord)\n",
    "            \n",
    "    cleaned.append(newSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@mayasolovely', ':', 'As', 'woman', 'complain', 'cleaning', 'house', '.', '&', 'man', 'always', 'take', 'trash', '...']\n",
      "24783\n"
     ]
    }
   ],
   "source": [
    "print(cleaned[0])\n",
    "print(len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxWords = max(map(len, cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(sent, vocab, maxWords):\n",
    "    n = len(sent)\n",
    "    numZeros = maxWords - n\n",
    "    result = [0]*numZeros\n",
    "    \n",
    "    for word in sent:\n",
    "        if word in vocab:\n",
    "            result.append(vocab[word] + 2)\n",
    "        else:\n",
    "            result.append(1)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(list(map(lambda x: create_seq(x, wordDict, maxWords), cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = np.zeros((len(y), 3))\n",
    "for i, l in enumerate(y):\n",
    "    y_cat[i, l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(sequences, y_cat, test_size = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41243\n"
     ]
    }
   ],
   "source": [
    "print(max(wordDict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Compiled...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 21188 samples, validate on 1116 samples\n",
      "Epoch 1/100\n",
      "21188/21188 [==============================] - 19s 889us/sample - loss: 0.1341 - val_loss: 0.1287\n",
      "Epoch 2/100\n",
      "21188/21188 [==============================] - 15s 722us/sample - loss: 0.1333 - val_loss: 0.1281\n",
      "Epoch 3/100\n",
      "21188/21188 [==============================] - 15s 710us/sample - loss: 0.1324 - val_loss: 0.1273\n",
      "Epoch 4/100\n",
      "21188/21188 [==============================] - 15s 714us/sample - loss: 0.1313 - val_loss: 0.1263\n",
      "Epoch 5/100\n",
      "21188/21188 [==============================] - 15s 712us/sample - loss: 0.1299 - val_loss: 0.1251\n",
      "Epoch 6/100\n",
      "21188/21188 [==============================] - 15s 716us/sample - loss: 0.1283 - val_loss: 0.1239\n",
      "Epoch 7/100\n",
      "21188/21188 [==============================] - 15s 712us/sample - loss: 0.1272 - val_loss: 0.1235\n",
      "Epoch 8/100\n",
      "21188/21188 [==============================] - 15s 705us/sample - loss: 0.1268 - val_loss: 0.1234\n",
      "Epoch 9/100\n",
      "21188/21188 [==============================] - 15s 707us/sample - loss: 0.1267 - val_loss: 0.1233\n",
      "Epoch 10/100\n",
      "21188/21188 [==============================] - 15s 729us/sample - loss: 0.1266 - val_loss: 0.1233\n",
      "Epoch 11/100\n",
      "21188/21188 [==============================] - 15s 710us/sample - loss: 0.1265 - val_loss: 0.1232\n",
      "Epoch 12/100\n",
      "21188/21188 [==============================] - 15s 710us/sample - loss: 0.1264 - val_loss: 0.1231\n",
      "Epoch 13/100\n",
      "21188/21188 [==============================] - 15s 708us/sample - loss: 0.1259 - val_loss: 0.1230\n",
      "Epoch 14/100\n",
      "21188/21188 [==============================] - 15s 704us/sample - loss: 0.1260 - val_loss: 0.1229\n",
      "Epoch 15/100\n",
      "21188/21188 [==============================] - 15s 714us/sample - loss: 0.1258 - val_loss: 0.1228\n",
      "Epoch 16/100\n",
      "21188/21188 [==============================] - 15s 709us/sample - loss: 0.1257 - val_loss: 0.1227\n",
      "Epoch 17/100\n",
      "21188/21188 [==============================] - 15s 710us/sample - loss: 0.1254 - val_loss: 0.1225\n",
      "Epoch 18/100\n",
      "21188/21188 [==============================] - 15s 709us/sample - loss: 0.1250 - val_loss: 0.1224\n",
      "Epoch 19/100\n",
      "21188/21188 [==============================] - 15s 705us/sample - loss: 0.1248 - val_loss: 0.1223\n",
      "Epoch 20/100\n",
      "21188/21188 [==============================] - 15s 708us/sample - loss: 0.1244 - val_loss: 0.1221\n",
      "Epoch 21/100\n",
      "21188/21188 [==============================] - 15s 711us/sample - loss: 0.1242 - val_loss: 0.1219\n",
      "Epoch 22/100\n",
      "21188/21188 [==============================] - 15s 723us/sample - loss: 0.1239 - val_loss: 0.1217\n",
      "Epoch 23/100\n",
      "21188/21188 [==============================] - 15s 706us/sample - loss: 0.1236 - val_loss: 0.1215\n",
      "Epoch 24/100\n",
      "21188/21188 [==============================] - 15s 709us/sample - loss: 0.1232 - val_loss: 0.1213\n",
      "Epoch 25/100\n",
      "21188/21188 [==============================] - 15s 715us/sample - loss: 0.1228 - val_loss: 0.1210\n",
      "Epoch 26/100\n",
      "21188/21188 [==============================] - 15s 716us/sample - loss: 0.1221 - val_loss: 0.1207\n",
      "Epoch 27/100\n",
      "21188/21188 [==============================] - 15s 717us/sample - loss: 0.1216 - val_loss: 0.1203\n",
      "Epoch 28/100\n",
      "21188/21188 [==============================] - 15s 716us/sample - loss: 0.1210 - val_loss: 0.1200\n",
      "Epoch 29/100\n",
      "21188/21188 [==============================] - 15s 715us/sample - loss: 0.1203 - val_loss: 0.1195\n",
      "Epoch 30/100\n",
      "21188/21188 [==============================] - 15s 715us/sample - loss: 0.1195 - val_loss: 0.1190\n",
      "Epoch 31/100\n",
      "21188/21188 [==============================] - 15s 714us/sample - loss: 0.1190 - val_loss: 0.1185\n",
      "Epoch 32/100\n",
      "21188/21188 [==============================] - 15s 716us/sample - loss: 0.1178 - val_loss: 0.1178\n",
      "Epoch 33/100\n",
      "21188/21188 [==============================] - 15s 704us/sample - loss: 0.1167 - val_loss: 0.1171\n",
      "Epoch 34/100\n",
      "21188/21188 [==============================] - 15s 709us/sample - loss: 0.1157 - val_loss: 0.1163\n",
      "Epoch 35/100\n",
      "21188/21188 [==============================] - 15s 716us/sample - loss: 0.1141 - val_loss: 0.1155\n",
      "Epoch 36/100\n",
      "21188/21188 [==============================] - 15s 708us/sample - loss: 0.1127 - val_loss: 0.1144\n",
      "Epoch 37/100\n",
      "21188/21188 [==============================] - 15s 710us/sample - loss: 0.1111 - val_loss: 0.1133\n",
      "Epoch 38/100\n",
      "21188/21188 [==============================] - 15s 714us/sample - loss: 0.1090 - val_loss: 0.1120\n",
      "Epoch 39/100\n",
      "21188/21188 [==============================] - 15s 708us/sample - loss: 0.1072 - val_loss: 0.1106\n",
      "Epoch 40/100\n",
      "21188/21188 [==============================] - 15s 709us/sample - loss: 0.1046 - val_loss: 0.1089\n",
      "Epoch 41/100\n",
      "21188/21188 [==============================] - 15s 714us/sample - loss: 0.1017 - val_loss: 0.1069\n",
      "Epoch 42/100\n",
      "21188/21188 [==============================] - 15s 714us/sample - loss: 0.0980 - val_loss: 0.1045\n",
      "Epoch 43/100\n",
      "21188/21188 [==============================] - 15s 709us/sample - loss: 0.0941 - val_loss: 0.1017\n",
      "Epoch 44/100\n",
      "21188/21188 [==============================] - 15s 703us/sample - loss: 0.0901 - val_loss: 0.0994\n",
      "Epoch 45/100\n",
      "21188/21188 [==============================] - 15s 701us/sample - loss: 0.0869 - val_loss: 0.0978\n",
      "Epoch 46/100\n",
      "21188/21188 [==============================] - 15s 712us/sample - loss: 0.0831 - val_loss: 0.0959\n",
      "Epoch 47/100\n",
      "21188/21188 [==============================] - 15s 703us/sample - loss: 0.0806 - val_loss: 0.0960\n",
      "Epoch 48/100\n",
      "21188/21188 [==============================] - 15s 704us/sample - loss: 0.0771 - val_loss: 0.0948\n",
      "Epoch 49/100\n",
      "21188/21188 [==============================] - 15s 708us/sample - loss: 0.0756 - val_loss: 0.0932\n",
      "Epoch 50/100\n",
      "21188/21188 [==============================] - 15s 718us/sample - loss: 0.0729 - val_loss: 0.0937\n",
      "Epoch 51/100\n",
      "21188/21188 [==============================] - 15s 729us/sample - loss: 0.0708 - val_loss: 0.0921\n",
      "Epoch 52/100\n",
      "21188/21188 [==============================] - 15s 725us/sample - loss: 0.0683 - val_loss: 0.0926\n",
      "Epoch 53/100\n",
      "21188/21188 [==============================] - 15s 713us/sample - loss: 0.0676 - val_loss: 0.0935\n",
      "Epoch 54/100\n",
      "21188/21188 [==============================] - 15s 708us/sample - loss: 0.0650 - val_loss: 0.0922\n"
     ]
    }
   ],
   "source": [
    "model = hate_speech_model()\n",
    "model.build_model()\n",
    "model.fit(X_train, y_train, epochs = 100, class_weight = {0: 1, 1: .05, 2: .15})\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8694688863729785\n"
     ]
    }
   ],
   "source": [
    "print(metrics.roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis = 1)\n",
    "y_pred = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.74      0.24       132\n",
      "           1       0.95      0.70      0.80      1959\n",
      "           2       0.71      0.61      0.66       388\n",
      "\n",
      "    accuracy                           0.69      2479\n",
      "   macro avg       0.60      0.68      0.57      2479\n",
      "weighted avg       0.87      0.69      0.75      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  98   22   12]\n",
      " [ 504 1371   84]\n",
      " [  97   55  236]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
