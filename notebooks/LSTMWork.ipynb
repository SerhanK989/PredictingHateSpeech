{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn import feature_extraction, linear_model, model_selection, metrics\n",
    "from sklearn import ensemble\n",
    "from scipy import sparse\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.make_model_lstm import hate_speech_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#watch overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweet'].apply(lambda x: x.lstrip('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(df[['hate_speech', 'offensive_language', 'neither']].values, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.tokenize.casual.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "wordDict = {}\n",
    "i = 1\n",
    "for tweet in tweets:\n",
    "    tokenized = token.tokenize(tweet)\n",
    "    newSent = []\n",
    "    for word in tokenized:\n",
    "        if word not in stop_words:\n",
    "            newWord = lemmatizer.lemmatize(word)\n",
    "            if newWord not in wordDict:\n",
    "                wordDict[newWord] = i\n",
    "                i += 1\n",
    "            newSent.append(newWord)\n",
    "            \n",
    "    cleaned.append(newSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@mayasolovely', ':', 'As', 'woman', 'complain', 'cleaning', 'house', '.', '&', 'man', 'always', 'take', 'trash', '...']\n",
      "24783\n"
     ]
    }
   ],
   "source": [
    "print(cleaned[0])\n",
    "print(len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxWords = max(map(len, cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(sent, vocab, maxWords):\n",
    "    n = len(sent)\n",
    "    numZeros = maxWords - n\n",
    "    result = [0]*numZeros\n",
    "    \n",
    "    for word in sent:\n",
    "        if word in vocab:\n",
    "            result.append(vocab[word] + 2)\n",
    "        else:\n",
    "            result.append(1)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(list(map(lambda x: create_seq(x, wordDict, maxWords), cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = np.zeros((len(y), 3))\n",
    "for i, l in enumerate(y):\n",
    "    y_cat[i, l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(sequences, y_cat, test_size = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41243\n"
     ]
    }
   ],
   "source": [
    "print(max(wordDict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Compiled...\n",
      "Train on 21188 samples, validate on 1116 samples\n",
      "Epoch 1/3\n",
      "21188/21188 [==============================] - 60s 3ms/sample - loss: 0.5791 - val_loss: 0.3730\n",
      "Epoch 2/3\n",
      "21188/21188 [==============================] - 57s 3ms/sample - loss: 0.2822 - val_loss: 0.3303\n",
      "Epoch 3/3\n",
      "21188/21188 [==============================] - 57s 3ms/sample - loss: 0.1713 - val_loss: 0.3560\n"
     ]
    }
   ],
   "source": [
    "model = hate_speech_model()\n",
    "model.build_model()\n",
    "model.fit(X_train, y_train, epochs = 3)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9100288670230242\n"
     ]
    }
   ],
   "source": [
    "print(metrics.roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis = 1)\n",
    "y_pred = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.17      0.24       145\n",
      "           1       0.90      0.95      0.92      1922\n",
      "           2       0.81      0.75      0.78       412\n",
      "\n",
      "    accuracy                           0.87      2479\n",
      "   macro avg       0.69      0.62      0.65      2479\n",
      "weighted avg       0.85      0.87      0.86      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  25  105   15]\n",
      " [  37 1828   57]\n",
      " [   5  100  307]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
